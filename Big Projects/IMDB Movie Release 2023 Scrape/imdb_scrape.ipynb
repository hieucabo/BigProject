{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url scrape\n",
    "url_base = r\"https://www.imdb.com/search/title/?title_type=feature&year=\"\n",
    "url_middle_base = r\"&sort=release_date,asc&start=\"\n",
    "url_last_base = r\"&ref_=adv_nxt\"\n",
    "\n",
    "# Dataframe columns\n",
    "columns = [\"title\",\"date\",\"run_time\",\"genre\",\"rating\",\"introduction\",\"director\",\"stars\",\"num_votes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "data_path = os.path.join(current_path, \"data\")\n",
    "\n",
    "already_scrape_day = []\n",
    "\n",
    "for path in os.listdir(data_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(data_path , path)):\n",
    "        already_scrape_day.append(path.split(\".\")[0])\n",
    "\n",
    "try:\n",
    "    next_scrape_day_date = datetime.strptime(max(already_scrape_day), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    next_scrape_day = next_scrape_day_date.strftime(\"%Y-%m-%d\")\n",
    "except: \n",
    "    next_scrape_day = \"2023-01-01\"\n",
    "\n",
    "# Date range to scrape\n",
    "date_range = pd.date_range(start = next_scrape_day, end = \"2023-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request html from the website\n",
    "def request(day,start_movie):\n",
    "    url_day = url_base + str(day.strftime(\"%Y-%m-%d\"))  + url_middle_base + str(start_movie) + url_last_base\n",
    "    html_data = requests.get(url_day).content.decode('utf-8')\n",
    "    soup = BeautifulSoup(html_data,'html5lib')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many movies released on that day\n",
    "def movie_by_day(soup):\n",
    "    try:\n",
    "        num_of_movie = soup.body.find_all(\"div\",{\"class\":\"desc\"})[0].find_all(\"span\")[0].text.split()[-2].replace(',', '')\n",
    "        total_movie = int(num_of_movie)\n",
    "    except: \n",
    "        total_movie = 0 \n",
    "    return total_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape title \n",
    "def scrape_title(content): \n",
    "    try:\n",
    "        title = content.find_all(\"h3\",{\"class\":\"lister-item-header\"})[0].find_all(\"a\")[0].text\n",
    "    except: \n",
    "        title = \"NA\"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape run time \n",
    "def scrape_run_time(content):\n",
    "    try:\n",
    "        run_time = content.find_all(\"p\",{\"class\":\"text-muted\"})[0].find_all(\"span\",{\"class\":\"runtime\"})[0].text.split()[0]\n",
    "    except:\n",
    "        run_time = \"NA\"\n",
    "    return run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape genre\n",
    "def scrape_genre(content):\n",
    "    try:\n",
    "        genre_name = content.find_all(\"p\",{\"class\":\"text-muted\"})[0].find_all(\"span\",{\"class\":\"genre\"})[0].text.strip()\n",
    "    except:\n",
    "        genre_name = \"NA\"\n",
    "    return genre_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape rating\n",
    "def scrape_rating(content):\n",
    "    try:\n",
    "        rating_imdb = content.find_all(\"div\",{\"class\":\"ratings-bar\"})[0].find_all(\"strong\")[0].text\n",
    "    except:\n",
    "        rating_imdb = \"NA\"\n",
    "    return rating_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape intro\n",
    "def scrape_introduction(content):\n",
    "    try:\n",
    "        intro = content.find_all(\"p\",{\"class\":\"text-muted\"})[1].text.strip().rstrip(\"See full summary\\xa0Â»\")\n",
    "        if intro == \"Add a Plot\": \n",
    "            intro = \"NA\"\n",
    "    except:\n",
    "        intro = \"NA\"\n",
    "    return intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape director\n",
    "def scrape_director(content):\n",
    "    try:\n",
    "        director_and_stars = content.find_all(\"p\",{\"class\":\"\"})[0].text.replace(\"\\n\",\"\").strip().split(\" | \")\n",
    "        director_name = [director for director in director_and_stars if \"Director\" in director][0].split(\":\")[1].strip()\n",
    "    except:\n",
    "        director_name = \"NA\"\n",
    "    return director_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape stars\n",
    "def scrape_stars(content):\n",
    "    try:\n",
    "        director_and_stars = content.find_all(\"p\",{\"class\":\"\"})[0].text.replace(\"\\n\",\"\").strip().split(\" | \")\n",
    "        stars_name = [stars for stars in director_and_stars if \"Star\" in stars][0].split(\":\")[1].strip()\n",
    "    except:\n",
    "        stars_name = \"NA\"\n",
    "    return stars_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape votes \n",
    "def scrape_votes(content):\n",
    "    try:\n",
    "        num_vote = content.find_all(\"p\",{\"class\":\"sort-num_votes-visible\"})[0].find_all(\"span\",{\"name\":\"nv\"})[0].text\n",
    "    except:\n",
    "        num_vote = \"NA\"\n",
    "    return num_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all data to row\n",
    "def merge_to_row(title,day,run_time,genre_name,rating_imdb,intro,director,star,vote):\n",
    "    row = pd.DataFrame({\"title\":title,\n",
    "                        \"date\":day,\n",
    "                        \"run_time\":run_time,\n",
    "                        \"genre\":genre_name,\n",
    "                        \"rating\":rating_imdb,\n",
    "                        \"introduction\":intro,\n",
    "                        \"director\":director,\n",
    "                        \"stars\":star,\n",
    "                        \"num_votes\":vote},\n",
    "                        columns = [\"title\",\"date\",\"run_time\",\"genre\",\"rating\",\"introduction\",\"director\",\"stars\",\"num_votes\"], index = [0]) \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the file and merge into one\n",
    "def concatenate_csv_files(data_path):\n",
    "    # Get a list of all CSV files in the input folder\n",
    "    csv_files = [file for file in os.listdir(data_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to store dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Read each CSV file into a DataFrame and add it to the list\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(data_path, csv_file)\n",
    "        df = pd.read_csv(csv_path, encoding = \"utf-16\")\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file\n",
    "    concatenated_df.to_csv(\"2023.csv\", index=False, encoding = \"utf-16\")\n",
    "    print(f\"All CSV files in '{data_path}' have been concatenated and saved to 2023.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish for day: 2023-05-24\n",
      "Finish for day: 2023-05-25\n",
      "Finish for day: 2023-05-26\n",
      "Finish for day: 2023-05-27\n",
      "Finish for day: 2023-05-28\n",
      "Finish for day: 2023-05-29\n",
      "Finish for day: 2023-05-30\n",
      "Finish for day: 2023-05-31\n",
      "Finish for day: 2023-06-01\n",
      "Finish for day: 2023-06-02\n",
      "Finish for day: 2023-06-03\n",
      "Finish for day: 2023-06-04\n",
      "Finish for day: 2023-06-05\n",
      "Finish for day: 2023-06-06\n",
      "Finish for day: 2023-06-07\n",
      "Finish for day: 2023-06-08\n",
      "Finish for day: 2023-06-09\n",
      "Finish for day: 2023-06-10\n",
      "Finish for day: 2023-06-11\n",
      "Finish for day: 2023-06-12\n",
      "Finish for day: 2023-06-13\n",
      "Finish for day: 2023-06-14\n",
      "Finish for day: 2023-06-15\n",
      "Finish for day: 2023-06-16\n",
      "Finish for day: 2023-06-17\n",
      "Finish for day: 2023-06-18\n",
      "Finish for day: 2023-06-19\n",
      "Finish for day: 2023-06-20\n",
      "Finish for day: 2023-06-21\n",
      "Finish for day: 2023-06-22\n",
      "Finish for day: 2023-06-23\n",
      "Finish for day: 2023-06-24\n",
      "Finish for day: 2023-06-25\n",
      "Finish for day: 2023-06-26\n",
      "Finish for day: 2023-06-27\n",
      "Finish for day: 2023-06-28\n",
      "Finish for day: 2023-06-29\n",
      "Finish for day: 2023-06-30\n",
      "Finish for day: 2023-07-01\n",
      "Finish for day: 2023-07-02\n",
      "Finish for day: 2023-07-03\n",
      "Finish for day: 2023-07-04\n",
      "Finish for day: 2023-07-05\n",
      "Finish for day: 2023-07-06\n",
      "Finish for day: 2023-07-07\n",
      "Finish for day: 2023-07-08\n",
      "Finish for day: 2023-07-09\n",
      "Finish for day: 2023-07-10\n",
      "Finish for day: 2023-07-11\n",
      "Finish for day: 2023-07-12\n",
      "Finish for day: 2023-07-13\n",
      "Finish for day: 2023-07-14\n",
      "Finish for day: 2023-07-15\n",
      "Finish for day: 2023-07-16\n",
      "Finish for day: 2023-07-17\n",
      "Finish for day: 2023-07-18\n",
      "Finish for day: 2023-07-19\n",
      "Finish for day: 2023-07-20\n",
      "Finish for day: 2023-07-21\n",
      "Finish for day: 2023-07-22\n",
      "Finish for day: 2023-07-23\n",
      "Finish for day: 2023-07-24\n",
      "Finish for day: 2023-07-25\n",
      "Finish for day: 2023-07-26\n",
      "Finish for day: 2023-07-27\n",
      "Finish for day: 2023-07-28\n",
      "Finish for day: 2023-07-29\n",
      "Finish for day: 2023-07-30\n",
      "Finish for day: 2023-07-31\n",
      "Finish for day: 2023-08-01\n",
      "Finish for day: 2023-08-02\n",
      "Finish for day: 2023-08-03\n",
      "Finish for day: 2023-08-04\n",
      "Finish for day: 2023-08-05\n",
      "Finish for day: 2023-08-06\n",
      "Finish for day: 2023-08-07\n",
      "Finish for day: 2023-08-08\n",
      "Finish for day: 2023-08-09\n",
      "Finish for day: 2023-08-10\n",
      "Finish for day: 2023-08-11\n",
      "Finish for day: 2023-08-12\n",
      "Finish for day: 2023-08-13\n",
      "Finish for day: 2023-08-14\n",
      "Finish for day: 2023-08-15\n",
      "Finish for day: 2023-08-16\n",
      "Finish for day: 2023-08-17\n",
      "Finish for day: 2023-08-18\n",
      "Finish for day: 2023-08-19\n",
      "Finish for day: 2023-08-20\n",
      "Finish for day: 2023-08-21\n",
      "Finish for day: 2023-08-22\n",
      "Finish for day: 2023-08-23\n",
      "Finish for day: 2023-08-24\n",
      "Finish for day: 2023-08-25\n",
      "Finish for day: 2023-08-26\n",
      "Finish for day: 2023-08-27\n",
      "Finish for day: 2023-08-28\n",
      "Finish for day: 2023-08-29\n",
      "Finish for day: 2023-08-30\n",
      "Finish for day: 2023-08-31\n",
      "Finish for day: 2023-09-01\n",
      "Finish for day: 2023-09-02\n",
      "Finish for day: 2023-09-03\n",
      "Finish for day: 2023-09-04\n",
      "Finish for day: 2023-09-05\n",
      "Finish for day: 2023-09-06\n",
      "Finish for day: 2023-09-07\n",
      "Finish for day: 2023-09-08\n",
      "Finish for day: 2023-09-09\n",
      "Finish for day: 2023-09-10\n",
      "Finish for day: 2023-09-11\n",
      "Finish for day: 2023-09-12\n",
      "Finish for day: 2023-09-13\n",
      "Finish for day: 2023-09-14\n",
      "Finish for day: 2023-09-15\n",
      "Finish for day: 2023-09-16\n",
      "Finish for day: 2023-09-17\n",
      "Finish for day: 2023-09-18\n",
      "Finish for day: 2023-09-19\n",
      "Finish for day: 2023-09-20\n",
      "Finish for day: 2023-09-21\n",
      "Finish for day: 2023-09-22\n",
      "Finish for day: 2023-09-23\n",
      "Finish for day: 2023-09-24\n",
      "Finish for day: 2023-09-25\n",
      "Finish for day: 2023-09-26\n",
      "Finish for day: 2023-09-27\n",
      "Finish for day: 2023-09-28\n",
      "Finish for day: 2023-09-29\n",
      "Finish for day: 2023-09-30\n",
      "Finish for day: 2023-10-01\n",
      "Finish for day: 2023-10-02\n",
      "Finish for day: 2023-10-03\n",
      "Finish for day: 2023-10-04\n",
      "Finish for day: 2023-10-05\n",
      "Finish for day: 2023-10-06\n",
      "Finish for day: 2023-10-07\n",
      "Finish for day: 2023-10-08\n",
      "Finish for day: 2023-10-09\n",
      "Finish for day: 2023-10-10\n",
      "Finish for day: 2023-10-11\n",
      "Finish for day: 2023-10-12\n",
      "Finish for day: 2023-10-13\n",
      "Finish for day: 2023-10-14\n",
      "Finish for day: 2023-10-15\n",
      "Finish for day: 2023-10-16\n",
      "Finish for day: 2023-10-17\n",
      "Finish for day: 2023-10-18\n",
      "Finish for day: 2023-10-19\n",
      "Finish for day: 2023-10-20\n",
      "Finish for day: 2023-10-21\n",
      "Finish for day: 2023-10-22\n",
      "Finish for day: 2023-10-23\n",
      "Finish for day: 2023-10-24\n",
      "Finish for day: 2023-10-25\n",
      "Finish for day: 2023-10-26\n",
      "Finish for day: 2023-10-27\n",
      "Finish for day: 2023-10-28\n",
      "Finish for day: 2023-10-29\n",
      "Finish for day: 2023-10-30\n",
      "Finish for day: 2023-10-31\n",
      "Finish for day: 2023-11-01\n",
      "Finish for day: 2023-11-02\n",
      "Finish for day: 2023-11-03\n",
      "Finish for day: 2023-11-04\n",
      "Finish for day: 2023-11-05\n",
      "Finish for day: 2023-11-06\n",
      "Finish for day: 2023-11-07\n",
      "Finish for day: 2023-11-08\n",
      "Finish for day: 2023-11-09\n",
      "Finish for day: 2023-11-10\n",
      "Finish for day: 2023-11-11\n",
      "Finish for day: 2023-11-12\n",
      "Finish for day: 2023-11-13\n",
      "Finish for day: 2023-11-14\n",
      "Finish for day: 2023-11-15\n",
      "Finish for day: 2023-11-16\n",
      "Finish for day: 2023-11-17\n",
      "Finish for day: 2023-11-18\n",
      "Finish for day: 2023-11-19\n",
      "Finish for day: 2023-11-20\n",
      "Finish for day: 2023-11-21\n",
      "Finish for day: 2023-11-22\n",
      "Finish for day: 2023-11-23\n",
      "Finish for day: 2023-11-24\n",
      "Finish for day: 2023-11-25\n",
      "Finish for day: 2023-11-26\n",
      "Finish for day: 2023-11-27\n",
      "Finish for day: 2023-11-28\n",
      "Finish for day: 2023-11-29\n",
      "Finish for day: 2023-11-30\n",
      "Finish for day: 2023-12-01\n",
      "Finish for day: 2023-12-02\n",
      "Finish for day: 2023-12-03\n",
      "Finish for day: 2023-12-04\n",
      "Finish for day: 2023-12-05\n",
      "Finish for day: 2023-12-06\n",
      "Finish for day: 2023-12-07\n",
      "Finish for day: 2023-12-08\n",
      "Finish for day: 2023-12-09\n",
      "Finish for day: 2023-12-10\n",
      "Finish for day: 2023-12-11\n",
      "Finish for day: 2023-12-12\n",
      "Finish for day: 2023-12-13\n",
      "Finish for day: 2023-12-14\n",
      "Finish for day: 2023-12-15\n",
      "Finish for day: 2023-12-16\n",
      "Finish for day: 2023-12-17\n",
      "Finish for day: 2023-12-18\n",
      "Finish for day: 2023-12-19\n",
      "Finish for day: 2023-12-20\n",
      "Finish for day: 2023-12-21\n",
      "Finish for day: 2023-12-22\n",
      "Finish for day: 2023-12-23\n",
      "Finish for day: 2023-12-24\n",
      "Finish for day: 2023-12-25\n",
      "Finish for day: 2023-12-26\n",
      "Finish for day: 2023-12-27\n",
      "Finish for day: 2023-12-28\n",
      "Finish for day: 2023-12-29\n",
      "Finish for day: 2023-12-30\n",
      "Finish for day: 2023-12-31\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe to store data \n",
    "for day in date_range:\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"title\",\"date\",\"run_time\",\"genre\",\"rating\",\"introduction\",\"director\",\"stars\",\"num_votes\"])\n",
    "\n",
    "    # Start page\n",
    "    current_page = 1 \n",
    "    start_movie = (current_page-1) * 50 + 1 \n",
    "\n",
    "    # request from the website\n",
    "    soup = request(day,start_movie)\n",
    "\n",
    "    # how many movies released on that day\n",
    "    num_of_movie = movie_by_day(soup)\n",
    "    \n",
    "    # how many page?\n",
    "    max_page = math.ceil(float(num_of_movie)/50)\n",
    "\n",
    "    # read the data of the page\n",
    "    for content in soup.find_all(\"div\",{\"class\":\"lister-item-content\"}):\n",
    "        title = scrape_title(content)\n",
    "        run_time = scrape_run_time(content)\n",
    "        genre_name = scrape_genre(content)\n",
    "        rating_imdb = scrape_rating(content) \n",
    "        intro = scrape_introduction(content)\n",
    "        director = scrape_director(content)\n",
    "        star = scrape_stars(content)\n",
    "        vote = scrape_votes(content)\n",
    "        row = merge_to_row(title,day,run_time,genre_name,rating_imdb,intro,director,star,vote)\n",
    "        df = pd.concat([df,row],ignore_index=True)\n",
    "\n",
    "    # if the max page > 1 then continue to scrape. After each page current_page increase by 1.\n",
    "    if max_page > 1: \n",
    "        current_page = current_page + 1 \n",
    "        while current_page <= max_page:\n",
    "            time.sleep(3)\n",
    "            start_movie = (current_page-1) * 50 + 1 \n",
    "            soup = request(day,start_movie)\n",
    "            for content in soup.find_all(\"div\",{\"class\":\"lister-item-content\"}):\n",
    "                title = scrape_title(content)\n",
    "                run_time = scrape_run_time(content)\n",
    "                genre_name = scrape_genre(content)\n",
    "                rating_imdb = scrape_rating(content) \n",
    "                intro = scrape_introduction(content)\n",
    "                director = scrape_director(content)\n",
    "                star = scrape_stars(content)\n",
    "                vote = scrape_votes(content)\n",
    "                row = merge_to_row(title,day,run_time,genre_name,rating_imdb,intro,director,star,vote)\n",
    "                df = pd.concat([df,row],ignore_index=True)\n",
    "            current_page = current_page + 1 \n",
    "    \n",
    "    #print current scraping day\n",
    "    df.to_csv(data_path +\"//\" + str(day.strftime(\"%Y-%m-%d\")) + \".csv\", encoding = \"utf-16\", index = None)\n",
    "    print(\"Finish for day: \" + str(day.strftime(\"%Y-%m-%d\")))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files in 'c:\\Users\\minhh\\Documents\\VSCode\\studysession\\Data Science\\imdb_scrpae\\data' have been concatenated and saved to 2023.csv!\n"
     ]
    }
   ],
   "source": [
    "concatenate_csv_files(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studysession",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
